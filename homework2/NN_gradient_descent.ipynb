{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification, make_regression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import linear_model, datasets\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import mlrose\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import randint\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoooda/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X,y= make_classification(n_samples=2000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, \n",
    "                            n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.2, class_sep=2.0, \n",
    "                            hypercube=True, shift=0.0, scale=1.0, shuffle=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "#preprocess data\n",
    "# Normalize feature data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# One hot encode target values\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "y_train_hot = one_hot.fit_transform(y_train.reshape(-1, 1)).todense()\n",
    "y_test_hot = one_hot.transform(y_test.reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = [100*i for i in range(20)]\n",
    "\n",
    "\n",
    "\n",
    "for itrial in range(4):\n",
    "    iterdata = []\n",
    "\n",
    "    for iteration in TRAINING_ITERATIONS:\n",
    "\n",
    "        seed=randint(1, 100)\n",
    "        # Initialize neural network object and fit object\n",
    "        start=time.time()\n",
    "        nn_model1 = mlrose.NeuralNetwork(hidden_nodes = [2], activation = 'relu',\n",
    "                                 algorithm = 'gradient_descent', max_iters = iteration,\n",
    "                                 bias = True, is_classifier = True, learning_rate = 0.1,\n",
    "                                 early_stopping = True, clip_max = 5, max_attempts = 1000,random_state = seed)\n",
    "\n",
    "        end=time.time()\n",
    "        nn_model1.fit(X_train_scaled, y_train_hot)\n",
    "\n",
    "        # Predict labels for train set and assess accuracy\n",
    "        y_train_pred = nn_model1.predict(X_train_scaled)        \n",
    "        y_train_accuracy = accuracy_score(y_train_hot, y_train_pred)\n",
    "        y_train_mse= mean_squared_error(y_train_hot, y_train_pred)\n",
    "        y_train_f1=f1_score(y_train_hot, y_train_pred,average='micro')\n",
    "        y_train_time=end-start\n",
    "        #print('Training accuracy: ', y_train_accuracy,y_train_mse,y_train_f1)\n",
    "\n",
    "\n",
    "\n",
    "        # Predict labels for test set and assess accuracy\n",
    "        y_test_pred = nn_model1.predict(X_test_scaled)   \n",
    "        y_test_accuracy = accuracy_score(y_test_hot, y_test_pred)\n",
    "        y_test_mse= mean_squared_error(y_test_hot, y_test_pred)\n",
    "        y_test_f1=f1_score(y_test_hot, y_test_pred,average='micro')\n",
    "        #print('Test accuracy: ', y_test_accuracy,y_test_mse,y_test_f1)   \n",
    "        \n",
    "        iterdata.append([iteration,y_train_accuracy,y_train_mse,y_train_f1,y_test_accuracy,y_test_mse,y_test_f1,y_train_time])\n",
    "        \n",
    "        \n",
    "    with open(\"NN_gd_{}.csv\".format(str(itrial)),'w') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        wr.writerow(['iteration','train_acc','train_mse','train_f1','test_acc','test_mse','test_f1','training_time'])\n",
    "        wr.writerows(iterdata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
